{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!unzip '*.zip'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ic4QfLT0NI6D",
    "outputId": "63d64ddf-d820-47af-be94-51b0fb422fa2"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  title_index.zip\n",
      "   creating: title_index/\n",
      "  inflating: title_index/title.pkl   \n",
      "  inflating: title_index/title_index_31_000.bin  \n",
      "  inflating: title_index/title_index_9_000.bin  \n",
      "  inflating: title_index/title_index_40_000.bin  \n",
      "  inflating: title_index/title_index_80_000.bin  \n",
      "  inflating: title_index/title_index_55_000.bin  \n",
      "  inflating: title_index/title_index_115_000.bin  \n",
      "  inflating: title_index/title_index_98_000.bin  \n",
      "  inflating: title_index/title_index_59_000.bin  \n",
      "  inflating: title_index/title_index_45_000.bin  \n",
      "\n",
      "Archive:  text_index.zip\n",
      "   creating: text_index/\n",
      "  inflating: text_index/text_index_59_000.bin  \n",
      "  inflating: text_index/text_index_108_000.bin  \n",
      "  inflating: text_index/text_index_50_000.bin  \n",
      "  inflating: text_index/text_index_42_000.bin  \n",
      "  inflating: text_index/text_index_101_000.bin  \n",
      "  inflating: text_index/text_index_76_000.bin  \n",
      "  inflating: text_index/text_index_47_000.bin  \n",
      "  inflating: text_index/text_index_1_000.bin  \n",
      "  inflating: text_index/text_index_6_000.bin  \n",
      "  inflating: text_index/text_index_44_000.bin  \n",
      "  inflating: text_index/text_index_37_000.bin  \n",
      "  inflating: text_index/text_index_86_000.bin  \n",
      "  inflating: text_index/text_index_104_000.bin  \n",
      "  inflating: text_index/text_index_33_000.bin  \n",
      "  inflating: text_index/text_index_41_000.bin  \n",
      "  inflating: text_index/text_index_11_000.bin  \n",
      "  inflating: text_index/text_index_111_000.bin  \n",
      "  inflating: text_index/text_index_35_000.bin  \n",
      "  inflating: text_index/text_index_91_000.bin  \n",
      "  inflating: text_index/text_index_68_000.bin  \n",
      "  inflating: text_index/text_index_55_000.bin  \n",
      "  inflating: text_index/text_index_63_000.bin  \n",
      "  inflating: text_index/text_index_95_000.bin  \n",
      "  inflating: text_index/text_index_39_000.bin  \n",
      "  inflating: text_index/text_index_98_000.bin  \n",
      "  inflating: text_index/text_index_88_000.bin  \n",
      "  inflating: text_index/text_index_38_000.bin  \n",
      "  inflating: text_index/text_index_106_000.bin  \n",
      "  inflating: text_index/text_index_9_000.bin  \n",
      "  inflating: text_index/text_index_94_000.bin  \n",
      "  inflating: text_index/text_index_92_000.bin  \n",
      "  inflating: text_index/text_index_48_000.bin  \n",
      "  inflating: text_index/text_index_43_000.bin  \n",
      "  inflating: text_index/text_index_17_000.bin  \n",
      "  inflating: text_index/text_index_18_000.bin  \n",
      "  inflating: text_index/text_index_16_000.bin  \n",
      "  inflating: text_index/text_index_60_000.bin  \n",
      "  inflating: text_index/text_index_81_000.bin  \n",
      "  inflating: text_index/text_index_84_000.bin  \n",
      "  inflating: text_index/text_index_116_000.bin  \n",
      "  inflating: text_index/text_index_25_000.bin  \n",
      "  inflating: text_index/text_index_112_000.bin  \n",
      "  inflating: text_index/text_index_49_000.bin  \n",
      "  inflating: text_index/text_index_62_000.bin  \n",
      "  inflating: text_index/text_index_15_000.bin  \n",
      "  inflating: text_index/text_index_28_000.bin  \n",
      "  inflating: text_index/text_index_79_000.bin  \n",
      "  inflating: text_index/text_index_26_000.bin  \n",
      "  inflating: text_index/text_index_99_000.bin  \n",
      "  inflating: text_index/text_index_27_000.bin  \n",
      "  inflating: text_index/text_index_78_000.bin  \n",
      "  inflating: text_index/text_index_12_000.bin  \n",
      "  inflating: text_index/text_index_93_000.bin  \n",
      "  inflating: text_index/text_index_58_000.bin  \n",
      "  inflating: text_index/text_index_66_000.bin  \n",
      "  inflating: text_index/text_index_19_000.bin  \n",
      "  inflating: text_index/text_index_57_000.bin  \n",
      "  inflating: text_index/text_index_65_000.bin  \n",
      "  inflating: text_index/text_index_70_000.bin  \n",
      "  inflating: text_index/text_index_105_000.bin  \n",
      "  inflating: text_index/text_index_3_000.bin  \n",
      "  inflating: text_index/text_index_29_000.bin  \n",
      "  inflating: text_index/text_index_113_000.bin  \n",
      "  inflating: text_index/text_index_85_000.bin  \n",
      "  inflating: text_index/text_index_83_000.bin  \n",
      "  inflating: text_index/text_index_117_000.bin  \n",
      "  inflating: text_index/text_index_89_000.bin  \n",
      "  inflating: text_index/text_index_54_000.bin  \n",
      "  inflating: text_index/text_index_73_000.bin  \n",
      "  inflating: text_index/text_index_120_000.bin  \n",
      "  inflating: text_index/text_index_20_000.bin  \n",
      "  inflating: text_index/text_index_51_000.bin  \n",
      "  inflating: text_index/text_index_100_000.bin  \n",
      "  inflating: text_index/text_index_7_000.bin  \n",
      "  inflating: text_index/text_index_34_000.bin  \n",
      "  inflating: text_index/text_index_31_000.bin  \n",
      "  inflating: text_index/text_index_75_000.bin  \n",
      "  inflating: text_index/text_index_80_000.bin  \n",
      "  inflating: text_index/text_index_0_000.bin  \n",
      "  inflating: text_index/text_index_64_000.bin  \n",
      "  inflating: text_index/text_index_45_000.bin  \n",
      "  inflating: text_index/text.pkl     \n",
      "  inflating: text_index/text_index_13_000.bin  \n",
      "  inflating: text_index/text_index_123_000.bin  \n",
      "  inflating: text_index/text_index_118_000.bin  \n",
      "  inflating: text_index/text_index_22_000.bin  \n",
      "  inflating: text_index/text_index_5_000.bin  \n",
      "  inflating: text_index/text_index_72_000.bin  \n",
      "  inflating: text_index/text_index_52_000.bin  \n",
      "  inflating: text_index/text_index_82_000.bin  \n",
      "  inflating: text_index/text_index_77_000.bin  \n",
      "  inflating: text_index/text_index_2_000.bin  \n",
      "  inflating: text_index/text_index_40_000.bin  \n",
      "  inflating: text_index/text_index_102_000.bin  \n",
      "  inflating: text_index/text_index_30_000.bin  \n",
      "  inflating: text_index/text_index_110_000.bin  \n",
      "  inflating: text_index/text_index_107_000.bin  \n",
      "  inflating: text_index/text_index_21_000.bin  \n",
      "  inflating: text_index/text_index_24_000.bin  \n",
      "  inflating: text_index/text_index_67_000.bin  \n",
      "  inflating: text_index/text_index_10_000.bin  \n",
      "  inflating: text_index/text_index_4_000.bin  \n",
      "  inflating: text_index/text_index_8_000.bin  \n",
      "  inflating: text_index/text_index_23_000.bin  \n",
      "  inflating: text_index/text_index_114_000.bin  \n",
      "  inflating: text_index/text_index_46_000.bin  \n",
      "  inflating: text_index/text_index_119_000.bin  \n",
      "  inflating: text_index/text_index_32_000.bin  \n",
      "  inflating: text_index/text_index_74_000.bin  \n",
      "  inflating: text_index/text_index_61_000.bin  \n",
      "  inflating: text_index/text_index_53_000.bin  \n",
      "  inflating: text_index/text_index_90_000.bin  \n",
      "  inflating: text_index/text_index_109_000.bin  \n",
      "  inflating: text_index/text_index_121_000.bin  \n",
      "  inflating: text_index/text_index_69_000.bin  \n",
      "  inflating: text_index/text_index_56_000.bin  \n",
      "  inflating: text_index/text_index_14_000.bin  \n",
      "  inflating: text_index/text_index_115_000.bin  \n",
      "  inflating: text_index/text_index_87_000.bin  \n",
      "  inflating: text_index/text_index_96_000.bin  \n",
      "  inflating: text_index/text_index_122_000.bin  \n",
      "  inflating: text_index/text_index_97_000.bin  \n",
      "  inflating: text_index/text_index_36_000.bin  \n",
      "  inflating: text_index/text_index_103_000.bin  \n",
      "  inflating: text_index/text_index_71_000.bin  \n",
      "\n",
      "Archive:  anchor_index.zip\n",
      "   creating: anchor_index/\n",
      "  inflating: anchor_index/anchor_index_42_000.bin  \n",
      "  inflating: anchor_index/anchor_index_56_000.bin  \n",
      "  inflating: anchor_index/anchor_index_60_000.bin  \n",
      "  inflating: anchor_index/anchor_index_121_000.bin  \n",
      "  inflating: anchor_index/anchor_index_113_000.bin  \n",
      "  inflating: anchor_index/anchor_index_37_000.bin  \n",
      "  inflating: anchor_index/anchor_index_112_000.bin  \n",
      "  inflating: anchor_index/anchor_index_0_000.bin  \n",
      "  inflating: anchor_index/anchor_index_72_000.bin  \n",
      "  inflating: anchor_index/anchor_index_66_000.bin  \n",
      "  inflating: anchor_index/anchor_index_44_000.bin  \n",
      "  inflating: anchor_index/anchor_index_49_000.bin  \n",
      "  inflating: anchor_index/anchor_index_73_000.bin  \n",
      "  inflating: anchor_index/anchor_index_17_000.bin  \n",
      "  inflating: anchor_index/anchor_index_21_000.bin  \n",
      "  inflating: anchor_index/anchor_index_67_000.bin  \n",
      "  inflating: anchor_index/anchor_index_11_000.bin  \n",
      "  inflating: anchor_index/anchor_index_88_000.bin  \n",
      "  inflating: anchor_index/anchor_index_10_000.bin  \n",
      "  inflating: anchor_index/anchor_index_51_000.bin  \n",
      "  inflating: anchor_index/anchor_index_4_000.bin  \n",
      "  inflating: anchor_index/anchor_index_16_000.bin  \n",
      "  inflating: anchor_index/anchor_index_86_000.bin  \n",
      "  inflating: anchor_index/anchor_index_2_000.bin  \n",
      "  inflating: anchor_index/anchor_index_55_000.bin  \n",
      "  inflating: anchor_index/anchor_index_96_000.bin  \n",
      "  inflating: anchor_index/anchor_index_87_000.bin  \n",
      "  inflating: anchor_index/anchor_index_61_000.bin  \n",
      "  inflating: anchor_index/anchor_index_109_000.bin  \n",
      "  inflating: anchor_index/anchor_index_75_000.bin  \n",
      "  inflating: anchor_index/anchor_index_43_000.bin  \n",
      "  inflating: anchor_index/anchor_index_84_000.bin  \n",
      "  inflating: anchor_index/anchor_index_36_000.bin  \n",
      "  inflating: anchor_index/anchor_index_105_000.bin  \n",
      "  inflating: anchor_index/anchor_index_29_000.bin  \n",
      "  inflating: anchor_index/anchor_index_50_000.bin  \n",
      "  inflating: anchor_index/anchor_index_82_000.bin  \n",
      "  inflating: anchor_index/anchor_index_111_000.bin  \n",
      "  inflating: anchor_index/anchor_index_47_000.bin  \n",
      "  inflating: anchor_index/anchor_index_54_000.bin  \n",
      "  inflating: anchor_index/anchor_index_98_000.bin  \n",
      "  inflating: anchor_index/anchor_index_38_000.bin  \n",
      "  inflating: anchor_index/anchor_index_118_000.bin  \n",
      "  inflating: anchor_index/anchor_index_63_000.bin  \n",
      "  inflating: anchor_index/anchor_index_97_000.bin  \n",
      "  inflating: anchor_index/anchor_index_64_000.bin  \n",
      "  inflating: anchor_index/anchor_index_90_000.bin  \n",
      "  inflating: anchor_index/anchor_index_48_000.bin  \n",
      "  inflating: anchor_index/anchor.pkl  \n",
      "  inflating: anchor_index/anchor_index_52_000.bin  \n",
      "  inflating: anchor_index/anchor_index_116_000.bin  \n",
      "  inflating: anchor_index/anchor_index_80_000.bin  \n",
      "  inflating: anchor_index/anchor_index_89_000.bin  \n",
      "  inflating: anchor_index/anchor_index_1_000.bin  \n",
      "  inflating: anchor_index/anchor_index_14_000.bin  \n",
      "  inflating: anchor_index/anchor_index_78_000.bin  \n",
      "  inflating: anchor_index/anchor_index_62_000.bin  \n",
      "  inflating: anchor_index/anchor_index_104_000.bin  \n",
      "  inflating: anchor_index/anchor_index_6_000.bin  \n",
      "  inflating: anchor_index/anchor_index_103_000.bin  \n",
      "  inflating: anchor_index/anchor_index_53_000.bin  \n",
      "  inflating: anchor_index/anchor_index_119_000.bin  \n",
      "  inflating: anchor_index/anchor_index_70_000.bin  \n",
      "  inflating: anchor_index/anchor_index_26_000.bin  \n",
      "  inflating: anchor_index/anchor_index_108_000.bin  \n",
      "  inflating: anchor_index/anchor_index_94_000.bin  \n",
      "  inflating: anchor_index/anchor_index_100_000.bin  \n",
      "  inflating: anchor_index/anchor_index_122_000.bin  \n",
      "  inflating: anchor_index/anchor_index_30_000.bin  \n",
      "  inflating: anchor_index/anchor_index_3_000.bin  \n",
      "  inflating: anchor_index/anchor_index_107_000.bin  \n",
      "  inflating: anchor_index/anchor_index_68_000.bin  \n",
      "  inflating: anchor_index/anchor_index_41_000.bin  \n",
      "  inflating: anchor_index/anchor_index_31_000.bin  \n",
      "  inflating: anchor_index/anchor_index_27_000.bin  \n",
      "  inflating: anchor_index/anchor_index_110_000.bin  \n",
      "  inflating: anchor_index/anchor_index_9_000.bin  \n",
      "  inflating: anchor_index/anchor_index_8_000.bin  \n",
      "  inflating: anchor_index/anchor_index_81_000.bin  \n",
      "  inflating: anchor_index/anchor_index_65_000.bin  \n",
      "  inflating: anchor_index/anchor_index_45_000.bin  \n",
      "  inflating: anchor_index/anchor_index_77_000.bin  \n",
      "  inflating: anchor_index/anchor_index_32_000.bin  \n",
      "  inflating: anchor_index/anchor_index_34_000.bin  \n",
      "  inflating: anchor_index/anchor_index_13_000.bin  \n",
      "  inflating: anchor_index/anchor_index_35_000.bin  \n",
      "  inflating: anchor_index/anchor_index_59_000.bin  \n",
      "  inflating: anchor_index/anchor_index_20_000.bin  \n",
      "  inflating: anchor_index/anchor_index_69_000.bin  \n",
      "  inflating: anchor_index/anchor_index_106_000.bin  \n",
      "  inflating: anchor_index/anchor_index_15_000.bin  \n",
      "  inflating: anchor_index/anchor_index_28_000.bin  \n",
      "  inflating: anchor_index/anchor_index_114_000.bin  \n",
      "  inflating: anchor_index/anchor_index_115_000.bin  \n",
      "  inflating: anchor_index/anchor_index_40_000.bin  \n",
      "  inflating: anchor_index/anchor_index_92_000.bin  \n",
      "  inflating: anchor_index/anchor_index_24_000.bin  \n",
      "  inflating: anchor_index/anchor_index_5_000.bin  \n",
      "  inflating: anchor_index/anchor_index_85_000.bin  \n",
      "  inflating: anchor_index/anchor_index_93_000.bin  \n",
      "  inflating: anchor_index/anchor_index_39_000.bin  \n",
      "  inflating: anchor_index/anchor_index_19_000.bin  \n",
      "  inflating: anchor_index/anchor_index_22_000.bin  \n",
      "  inflating: anchor_index/anchor_index_71_000.bin  \n",
      "  inflating: anchor_index/anchor_index_95_000.bin  \n",
      "  inflating: anchor_index/anchor_index_74_000.bin  \n",
      "  inflating: anchor_index/anchor_index_83_000.bin  \n",
      "  inflating: anchor_index/anchor_index_117_000.bin  \n",
      "  inflating: anchor_index/anchor_index_99_000.bin  \n",
      "  inflating: anchor_index/anchor_index_79_000.bin  \n",
      "  inflating: anchor_index/anchor_index_25_000.bin  \n",
      "  inflating: anchor_index/anchor_index_33_000.bin  \n",
      "  inflating: anchor_index/anchor_index_120_000.bin  \n",
      "  inflating: anchor_index/anchor_index_23_000.bin  \n",
      "  inflating: anchor_index/anchor_index_76_000.bin  \n",
      "  inflating: anchor_index/anchor_index_57_000.bin  \n",
      "  inflating: anchor_index/anchor_index_123_000.bin  \n",
      "  inflating: anchor_index/anchor_index_58_000.bin  \n",
      "  inflating: anchor_index/anchor_index_12_000.bin  \n",
      "  inflating: anchor_index/anchor_index_102_000.bin  \n",
      "  inflating: anchor_index/anchor_index_46_000.bin  \n",
      "  inflating: anchor_index/anchor_index_18_000.bin  \n",
      "  inflating: anchor_index/anchor_index_101_000.bin  \n",
      "  inflating: anchor_index/anchor_index_7_000.bin  \n",
      "  inflating: anchor_index/anchor_index_91_000.bin  \n",
      "\n",
      "3 archives were successfully processed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# from collections import Counter, OrderedDict\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from inverted_index_colab_edited import *"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vduzsUm1Nmkk",
    "outputId": "ba2ec955-f2b5-4ae3-c2e5-42ee012be028"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# These will already be installed in the testing environment so disregard the\n",
    "# amount of time (~1 minute) it takes to install.\n",
    "!pip install -q pyspark\n",
    "!pip install -U -q PyDrive\n",
    "!apt install openjdk-8-jdk-headless -qq\n",
    "!pip install -q graphframes\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
    "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
    "!wget -N -P $spark_jars $graphframes_jar"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93IgKj6CPB_z",
    "outputId": "2b476913-2011-4712-af01-281df5ccca94"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The following additional packages will be installed:\n",
      "  libxtst6 openjdk-8-jre-headless\n",
      "Suggested packages:\n",
      "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
      "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
      "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 39.7 MB of archives.\n",
      "After this operation, 144 MB of additional disk space will be used.\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "(Reading database ... 124565 files and directories currently installed.)\n",
      "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u432-ga~us1-0ubuntu2~22.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u432-ga~us1-0ubuntu2~22.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.7/154.7 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h--2025-01-16 08:35:54--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
      "Resolving repos.spark-packages.org (repos.spark-packages.org)... 52.84.125.114, 52.84.125.113, 52.84.125.73, ...\n",
      "Connecting to repos.spark-packages.org (repos.spark-packages.org)|52.84.125.114|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 247880 (242K) [binary/octet-stream]\n",
      "Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
      "\n",
      "graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-01-16 08:35:54 (5.71 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from graphframes import *"
   ],
   "metadata": {
    "id": "nBh8rnT4PFe0"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initializing spark context\n",
    "# create a spark context and session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "id": "sqP3niCBPILT"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "rVYV1VojPKCE",
    "outputId": "c556f869-79cf-4bd7-9b7a-57420f050456"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7edbb8956590>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://28c5e9627d38:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Authenticate your user\n",
    "# The authentication should be done with the email connected to your GCP account\n",
    "from google.colab import auth\n",
    "import signal\n",
    "\n",
    "AUTH_TIMEOUT = 100\n",
    "\n",
    "def handler(signum, frame):\n",
    "  raise Exception(\"Authentication timeout!\")\n",
    "\n",
    "try:\n",
    "  signal.signal(signal.SIGALRM, handler)\n",
    "  signal.alarm(AUTH_TIMEOUT)\n",
    "  auth.authenticate_user()\n",
    "  signal.alarm(0)\n",
    "except:\n",
    "  pass"
   ],
   "metadata": {
    "id": "QhcEt8P6PM55"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import auth\n",
    "from google.cloud import storage\n",
    "\n",
    "# Alternative authentication method\n",
    "!gcloud auth login\n",
    "\n",
    "# Set your project and bucket info\n",
    "project_id = 'hw2-project-444515'\n",
    "data_bucket_name = '314616475'\n",
    "\n",
    "# Configure project\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "# Create directory and copy file directly\n",
    "try:\n",
    "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
    "        pass\n",
    "except:\n",
    "    !mkdir -p wikidumps\n",
    "    !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDaY3KbAPPeq",
    "outputId": "6438523e-e43c-4a9a-8aea-095d55bf5b5a"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "You are running on a Google Compute Engine virtual machine.\n",
      "It is recommended that you use service accounts for authentication.\n",
      "\n",
      "You can run:\n",
      "\n",
      "  $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "to switch accounts if necessary.\n",
      "\n",
      "Your credentials may be visible to others with access to this\n",
      "virtual machine. Are you sure you want to authenticate with\n",
      "your personal account?\n",
      "\n",
      "Do you want to continue (Y/n)?  y\n",
      "\n",
      "Go to the following link in your browser, and complete the sign-in prompts:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=Z2tHso5BQw093ZVsRd0LOo4K8lMUSJ&prompt=consent&token_usage=remote&access_type=offline&code_challenge=gk9dWOmYVNln0t6IV84X5pfTITvd_k0xJ2CAbFnso4s&code_challenge_method=S256\n",
      "\n",
      "Once finished, enter the verification code provided in your browser: 4/0AanRRrt4v09OwCDDu1u_7YqgpF5Fby1y1uqRmofgxJAxVWPOCne3lMo9tujPtKGC00__Cw\n",
      "\n",
      "You are now logged in as [shhara@post.bgu.ac.il].\n",
      "Your current project is [None].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "Updated property [core/project].\n",
      "Copying gs://314616475/multistream1_preprocessed.parquet...\n",
      "==> NOTE: You are downloading one or more large file(s), which would\n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "\\ [1 files][316.7 MiB/316.7 MiB]                                                \n",
      "Operation completed over 1 objects/316.7 MiB.                                    \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
    "      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
    "except:\n",
    "      path = \"wikidumps/*\"\n",
    "\n",
    "parquetFile = spark.read.parquet(path)\n",
    "parquetFile.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8wqbfdnPUOT",
    "outputId": "a0e46f95-6701-4dfe-f8d5-9301cac47f4f"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               title|                text|         anchor_text|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "| 12|           Anarchism|'''Anarchism''' i...|[{23040, politica...|\n",
      "| 25|              Autism|'''Autism''' is a...|[{492271, Clinica...|\n",
      "| 39|              Albedo|thumb|upright=1.3...|[{679294, diffuse...|\n",
      "|290|                   A|'''A''', or '''a'...|[{290, See below}...|\n",
      "|303|             Alabama|'''Alabama''' () ...|[{351590, Yellowh...|\n",
      "|305|            Achilles|thumb|260px|Ancie...|[{1076007, potter...|\n",
      "|307|     Abraham Lincoln|'''Abraham Lincol...|[{1827174, Alexan...|\n",
      "|308|           Aristotle|'''Aristotle''' (...|[{1389981, bust},...|\n",
      "|309|An American in Paris|'''''An American ...|[{13066, George G...|\n",
      "|316|Academy Award for...|The '''Academy Aw...|[{39842, Academy ...|\n",
      "|324|      Academy Awards|The '''Academy Aw...|[{649481, film in...|\n",
      "|330|             Actrius|'''''Actresses'''...|[{5282, Catalan},...|\n",
      "|332|     Animalia (book)|'''''Animalia''''...|[{2511084, Graeme...|\n",
      "|334|International Ato...|'''International ...|[{25453985, atomi...|\n",
      "|336|            Altruism|thumb|Giving alms...|[{657573, alms}, ...|\n",
      "|339|            Ayn Rand|'''Alice O'Connor...|[{24320051, St. P...|\n",
      "|340|        Alain Connes|'''Alain Connes''...|[{1201522, Dragui...|\n",
      "|344|          Allan Dwan|'''Allan Dwan''' ...|[{64646, Toronto}...|\n",
      "|358|             Algeria|'''Algeria''', of...|[{803, Arabic}, {...|\n",
      "|359|List of Atlas Shr...|This is a list of...|[{339, Ayn Rand},...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# take the 'text' and 'id' or the first 1000 rows and create an RDD from it\n",
    "doc_text_pairs = parquetFile.limit(1000).select(\"text\", \"id\").rdd\n",
    "doc_title_pairs = parquetFile.limit(1000).select(\"title\", \"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.limit(1000).select(\"anchor_text\", \"id\").rdd"
   ],
   "metadata": {
    "id": "tzDMbRvwPon6"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "def tokenize(text):\n",
    "  english_stopwords = frozenset(stopwords.words('english'))\n",
    "  corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
    "  RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "  all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  stemmed = [stemmer.stem(token) for token in tokens if token not in all_stopwords]\n",
    "  return stemmed\n",
    "\n",
    "def stemmed_word_count(text, id):\n",
    "  ''' Count the frequency of each word in `text` (tf) that is not included in\n",
    "  `all_stopwords` and return entries that will go into our posting lists.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "  Returns:\n",
    "  --------\n",
    "    List of tuples\n",
    "      A list of (token, (doc_id, tf)) pairs\n",
    "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "  '''\n",
    "  tokens = tokenize(text)\n",
    "  token_counts = Counter(tokens)  # Calculate term frequencies efficiently\n",
    "  res_list = [\n",
    "        (token, (id, count))\n",
    "        for token, count in token_counts.items()\n",
    "        if token not in all_stopwords\n",
    "    ]\n",
    "  return res_list\n",
    "\n",
    "\n",
    "def len_tokens(text, id):\n",
    "  english_stopwords = frozenset(stopwords.words('english'))\n",
    "  corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
    "  RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "  all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  tokens = [x for x in tokens if x not in all_stopwords]\n",
    "  return (id, len(tokens))"
   ],
   "metadata": {
    "id": "6KPStQl6Psnc"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_doc_lengths_dict = {}\n",
    "title_doc_lengths_dict = {}\n",
    "\n",
    "\n",
    "# Apply the function to your RDD\n",
    "text_len_pairs = doc_text_pairs.map(lambda x: len_tokens(x[0], x[1]))\n",
    "text_doc_lengths_dict = text_len_pairs.collectAsMap()\n",
    "\n",
    "title_len_pairs = doc_title_pairs.map(lambda x: len_tokens(x[0], x[1]))\n",
    "title_doc_lengths_dict = title_len_pairs.collectAsMap()"
   ],
   "metadata": {
    "id": "83auTTbAP8US"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from itertools import chain\n",
    "import time\n",
    "import builtins\n",
    "\n",
    "\n",
    "# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n",
    "class BM25_from_index:\n",
    "    \"\"\"\n",
    "    Best Match 25.\n",
    "    ----------\n",
    "    k1 : float, default 1.5\n",
    "\n",
    "    b : float, default 0.75\n",
    "\n",
    "    index: inverted index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,index,DL,k1=1.5, b=0.75):\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "        self.index = index\n",
    "        self.DL = DL\n",
    "        self.N = len(DL)\n",
    "        # self.AVGDL = sum(DL.values())/self.N\n",
    "        # self.AVGDL = sum(list(DL.values())) / self.N\n",
    "        self.AVGDL = builtins.sum(DL.values()) / self.N\n",
    "        self.words, self.pls = zip(*self.index.posting_lists_iter())\n",
    "\n",
    "    def calc_idf(self,list_of_tokens):\n",
    "        \"\"\"\n",
    "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        idf: dictionary of idf scores. As follows:\n",
    "                                                    key: term\n",
    "                                                    value: bm25 idf score\n",
    "        \"\"\"\n",
    "        idf = {}\n",
    "        for term in list_of_tokens:\n",
    "            if term in self.index.df.keys():\n",
    "                n_ti = self.index.df[term]\n",
    "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
    "            else:\n",
    "                pass\n",
    "        return idf\n",
    "\n",
    "\n",
    "    def search(self, queries,N=3):\n",
    "        \"\"\"\n",
    "        This function calculate the bm25 score for given query and document.\n",
    "        We need to check only documents which are 'candidates' for a given query.\n",
    "        This function return a dictionary of scores as the following:\n",
    "                                                                    key: query_id\n",
    "                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "        doc_id: integer, document id.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        score: float, bm25 score.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        query_scores = {} # init dict\n",
    "\n",
    "        for query_id, query_tokens in queries.items():\n",
    "            self.idf = self.calc_idf(query_tokens) # calc idf for current query\n",
    "\n",
    "            if not self.idf:\n",
    "                query_scores[query_id] = []  # no relevant documents\n",
    "                continue\n",
    "\n",
    "            # getting candidtaes\n",
    "            candidate_docs = set()\n",
    "            for term in query_tokens:\n",
    "                if term in self.index.df:\n",
    "                    posting_list = self.pls[self.words.index(term)]\n",
    "                    # print(f\"posting_list for term {term} is:{posting_list}\")\n",
    "                    candidate_docs.update([doc_id for doc_id, _ in posting_list])\n",
    "                    # print(f\"candidate_docs for term {term} are: {candidate_docs}\")\n",
    "\n",
    "            # calc common terms' data\n",
    "            common_terms_data = {}\n",
    "            for term in query_tokens:\n",
    "                if term in self.index.df:\n",
    "                    posting_list = self.pls[self.words.index(term)]\n",
    "                    common_terms_data[term] = dict(posting_list)\n",
    "\n",
    "            # calc scores for candidates\n",
    "            doc_scores = {}\n",
    "            for doc_id in candidate_docs:\n",
    "                score = 0.0\n",
    "                doc_len = self.DL[doc_id]\n",
    "\n",
    "                for term in query_tokens:\n",
    "                    if term in common_terms_data and doc_id in common_terms_data[term]:\n",
    "                        freq = common_terms_data[term][doc_id]\n",
    "                        numerator = self.idf[term] * freq * (self.k1 + 1)\n",
    "                        denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
    "                        score += (numerator / denominator)\n",
    "\n",
    "                doc_scores[doc_id] = score\n",
    "\n",
    "            sorted_scores = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "            top_n_docs = sorted_scores[:N]\n",
    "\n",
    "            query_scores[query_id] = top_n_docs\n",
    "\n",
    "        return query_scores\n",
    "\n",
    "    def _score(self, query, doc_id):\n",
    "        \"\"\"\n",
    "        This function calculate the bm25 score for given query and document.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "        doc_id: integer, document id.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        score: float, bm25 score.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        doc_len = DL[str(doc_id)]\n",
    "\n",
    "        for term in query:\n",
    "            if term in self.index.term_total.keys():\n",
    "                term_frequencies = dict(self.pls[self.words.index(term)])\n",
    "                if doc_id in term_frequencies.keys():\n",
    "                    freq = term_frequencies[doc_id]\n",
    "                    numerator = self.idf[term] * freq * (self.k1 + 1)\n",
    "                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
    "                    score += (numerator / denominator)\n",
    "        return score"
   ],
   "metadata": {
    "id": "x8AqhUW_N3Nq"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "idx_text = InvertedIndex.read_index('text_index', 'text')\n",
    "idx_title = InvertedIndex.read_index('title_index', 'title')\n",
    "idx_anchor = InvertedIndex.read_index('anchor_index', 'anchor')"
   ],
   "metadata": {
    "id": "SmLQJDSsN8qS"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bm25_text = BM25_from_index(idx_text, text_doc_lengths_dict)\n",
    "bm25_title = BM25_from_index(idx_title, title_doc_lengths_dict)\n",
    "bm25_anchor = BM25_from_index(idx_anchor, text_doc_lengths_dict)  # this needed to be changed! wrong DL"
   ],
   "metadata": {
    "id": "cqY1TolUN-RK"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pages_links = spark.read.parquet(path).limit(1000).select(\"id\", \"anchor_text\").rdd"
   ],
   "metadata": {
    "id": "sotDMpq0RcK6"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Collect IDs from both RDDs\n",
    "ids_pages_links = set(pages_links.map(lambda row: row.id).collect())\n",
    "ids_doc_anchor_pairs = set(doc_anchor_pairs.map(lambda row: row.id).collect())\n",
    "\n",
    "# Check if they are the same\n",
    "ids_pages_links == ids_doc_anchor_pairs\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUP4qP1NR66T",
    "outputId": "753b1ad7-a140-47c7-93e7-6e2b113abbbe"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_graph(pages):\n",
    "  ''' Compute the directed graph generated by wiki links.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    pages: RDD\n",
    "      An RDD where each row consists of one wikipedia articles with 'id' and\n",
    "      'anchor_text'.\n",
    "  Returns:\n",
    "  --------\n",
    "    edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the\n",
    "      second entry is the destination page id. No duplicates should be present.\n",
    "    vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph\n",
    "      created by the wikipedia links. No duplicates should be present.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  # raise NotImplementedError()\n",
    "\n",
    "  # edges = pages.flatMap(lambda x: [(x[0], anchor) for anchor in x[1]])\n",
    "  # edges = edges.mapValues(lambda x: (x[0],))\n",
    "  edges = pages.flatMap(lambda x: [(x[0], anchor[0]) for anchor in x[1]])\n",
    "  edges = edges.distinct()\n",
    "\n",
    "  # vertices = edges.flatMap(lambda x:  [(x[0],), (x[1],)])\n",
    "  # vertices = edges.flatMap(lambda x: [x[0], x[1][0]]).distinct()\n",
    "  vertices = edges.flatMap(lambda x: [x[0], x[1]]).distinct()\n",
    "  vertices = vertices.map(lambda x: Row(id=x))\n",
    "\n",
    "  # vertices = vertices.map(lambda x: Row(id=x))\n",
    "  # vertices = vertices.distinct()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return edges, vertices"
   ],
   "metadata": {
    "id": "c0oYW3yxSASL"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "edges, vertices = generate_graph(pages_links)"
   ],
   "metadata": {
    "id": "5xZv_02TSIA6"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "pr.repartition(1).write.csv('pr', compression=\"gzip\")\n",
    "pr.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tY-HlqysSMET",
    "outputId": "81f66a2b-0a02-4edc-e8d8-ca14e905831d"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------+\n",
      "|      id|          pagerank|\n",
      "+--------+------------------+\n",
      "|  606848|1.9768910954777374|\n",
      "|43432956|1.7183234055660317|\n",
      "|37404646| 1.561180019985396|\n",
      "|  148363| 1.531189053212204|\n",
      "|     308| 1.523973241896932|\n",
      "|     783|1.5205983727677175|\n",
      "|   39235|1.5113438223085562|\n",
      "|   30680| 1.506487061855029|\n",
      "| 3434750|1.5042432651671351|\n",
      "|   48518|1.5032093881505733|\n",
      "| 5843419| 1.475564533072746|\n",
      "|   25458|1.4651925575346503|\n",
      "|     854| 1.433614708798739|\n",
      "|   14532|1.4318560075868727|\n",
      "|   11887| 1.422942493084949|\n",
      "|   32927|1.4195131540605397|\n",
      "| 8034453| 1.414325748041313|\n",
      "|    1686|1.4005429901986137|\n",
      "|   25507|1.3695924013144563|\n",
      "|   61371|1.3682656561066355|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):\n",
    "    \"\"\"\n",
    "    This function merge and sort documents retrieved by its weighte score (e.g., title and body).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows:\n",
    "                                                                            key: query_id\n",
    "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
    "\n",
    "    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows:\n",
    "                                                                            key: query_id\n",
    "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
    "    title_weight: float, for weigted average utilizing title and body scores\n",
    "    text_weight: float, for weigted average utilizing title and body scores\n",
    "    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    dictionary of querires and topN pairs as follows:\n",
    "                                                        key: query_id\n",
    "                                                        value: list of pairs in the following format:(doc_id,score).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    merged_results = {} # init dict\n",
    "\n",
    "    for query_id in title_scores:\n",
    "        title_results = {doc_id: score for doc_id, score in title_scores.get(query_id, [])} # scores for titles\n",
    "        body_results = {doc_id: score for doc_id, score in body_scores.get(query_id, [])} # scores for body\n",
    "\n",
    "        all_doc_ids = set(title_results.keys()) | set(body_results.keys()) # Get all unique document IDs\n",
    "\n",
    "        # calc combined scores\n",
    "        combined_scores = {}\n",
    "        for doc_id in all_doc_ids:\n",
    "            title_score = title_results.get(doc_id, 0)\n",
    "            body_score = body_results.get(doc_id, 0)\n",
    "            combined_score = (title_score * title_weight) + (body_score * text_weight)\n",
    "            combined_scores[doc_id] = combined_score\n",
    "\n",
    "        sorted_scores = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_n_docs = sorted_scores[:N]\n",
    "\n",
    "        merged_results[query_id] = top_n_docs\n",
    "\n",
    "    return merged_results"
   ],
   "metadata": {
    "id": "AR0MdeyxUyMD"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "queries = {\n",
    "    1: \"What is the role of feminism in modern society?\",\n",
    "    2: \"Explain the causes and effects of the 9th-century revolutions.\",\n",
    "    3: \"How did Lenin influence Soviet philosophy?\",\n",
    "    4: \"What are the properties of genetic mutations in the human genome?\",\n",
    "    5: \"Discuss the architectural significance of downtown skyscrapers.\",\n",
    "    6: \"What were the primary causes of the Great Fire of London?\",\n",
    "    7: \"Analyze the impact of postmodern art on 20th-century culture.\",\n",
    "    8: \"What advancements in aerodynamics have improved airplane design?\",\n",
    "    9: \"How does RNA differ from DNA in its function?\",\n",
    "    10: \"Provide an overview of the contributions of Albert Einstein to physics.\",\n",
    "}\n",
    "\n",
    "tokenized_queries = {qid: tokenize(query) for qid, query in queries.items()}\n",
    "\n",
    "def tokenize_query(query):\n",
    "    return tokenize(query)\n",
    "\n",
    "tokens_query1 = tokenize_query(queries[1])\n",
    "tokens_query2 = tokenize_query(queries[2])\n",
    "\n",
    "bm25_queries_score_train_text = bm25_text.search({1: tokens_query1},N=10)\n",
    "for score in bm25_queries_score_train_text.values():\n",
    "    print(score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fG3rFoDZV_kE",
    "outputId": "31cdf13d-4628-4ea9-8f2c-6b8b25cc267a"
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(573, 5.76128035298307), (12, 5.661777336976286), (1546, 5.653170053129046), (1494, 5.482040916127122), (1423, 5.421440686956775), (569, 5.367232607706526), (1214, 5.333890378050214), (2236, 5.263435513900069), (1135, 5.215230332177688), (2274, 5.207244135038309)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_candidates(query, index):\n",
    "  '''\n",
    "    function that gets tokenized query and returns a set of candidate documents for retrieval\n",
    "  '''\n",
    "  words, pls = zip(*index.posting_lists_iter())\n",
    "  candidate_docs = set()\n",
    "  for term in query:\n",
    "      if term in index.df:\n",
    "          try:\n",
    "              # Find the index of the term in the 'words' tuple\n",
    "              term_index = words.index(term)\n",
    "              # Use the index to access the posting list\n",
    "              posting_list = pls[term_index]\n",
    "              candidate_docs.update([doc_id for doc_id, _ in posting_list])\n",
    "          except ValueError:\n",
    "              # Handle the case where the term is not found in 'words'\n",
    "              pass  # Or print a message, or raise an exception, depending on your needs\n",
    "  return candidate_docs\n",
    "\n",
    "\n",
    "print(get_candidates(tokens_query1, idx_text))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_9213gWfPLm",
    "outputId": "bbd948c5-8669-49e6-8282-e7350a7e0f0b"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{2052, 2053, 12, 2062, 2063, 2064, 2065, 2067, 2070, 25, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 39, 2088, 2101, 2104, 2108, 2112, 2113, 2114, 2115, 2116, 2118, 2122, 2126, 2130, 2134, 2136, 2137, 2140, 2141, 2142, 2144, 2147, 2148, 2151, 2152, 2154, 2162, 2167, 2171, 2174, 2175, 2176, 2178, 2179, 2180, 2185, 2187, 2192, 2194, 2202, 2204, 2207, 2208, 2209, 2210, 2215, 2216, 2217, 2218, 2219, 2224, 2226, 2233, 2234, 2236, 2237, 2238, 2241, 2244, 2250, 2251, 2257, 2268, 2273, 2274, 2282, 2284, 2286, 2287, 2288, 2296, 2299, 2303, 2304, 2308, 2310, 2314, 2315, 2321, 2322, 2323, 2328, 2333, 2335, 2338, 290, 2341, 2345, 2349, 303, 305, 307, 308, 2357, 2358, 309, 2362, 2363, 316, 2369, 2371, 324, 2376, 2377, 330, 2380, 2382, 2384, 336, 2386, 339, 2388, 340, 2391, 344, 2393, 2396, 2397, 2398, 2400, 2402, 2403, 358, 359, 2408, 2406, 2411, 2414, 2416, 2417, 2422, 2425, 2427, 2428, 2431, 2433, 2439, 2440, 2444, 2447, 2448, 2457, 569, 572, 573, 580, 586, 593, 594, 597, 599, 600, 620, 621, 624, 627, 628, 633, 639, 653, 655, 656, 657, 659, 662, 664, 665, 666, 670, 673, 674, 676, 677, 680, 682, 683, 689, 691, 698, 700, 701, 704, 706, 709, 710, 711, 713, 717, 736, 737, 738, 740, 746, 748, 751, 752, 765, 771, 772, 775, 782, 783, 784, 785, 786, 791, 795, 798, 799, 800, 802, 803, 808, 824, 841, 842, 843, 844, 846, 848, 849, 851, 852, 854, 856, 857, 863, 864, 869, 872, 874, 877, 878, 880, 887, 888, 890, 894, 897, 898, 900, 901, 902, 903, 904, 905, 909, 910, 914, 922, 928, 929, 930, 931, 951, 953, 954, 956, 958, 960, 974, 980, 981, 983, 984, 986, 1000, 1002, 1004, 1005, 1006, 1008, 1012, 1013, 1014, 1016, 1017, 1020, 1022, 1023, 1028, 1029, 1030, 1032, 1038, 1049, 1051, 1055, 1058, 1063, 1064, 1069, 1074, 1078, 1081, 1088, 1091, 1092, 1093, 1094, 1097, 1110, 1130, 1134, 1135, 1136, 1140, 1141, 1143, 1144, 1146, 1148, 1152, 1154, 1162, 1164, 1166, 1167, 1168, 1170, 1171, 1174, 1175, 1177, 1178, 1181, 1182, 1183, 1187, 1192, 1193, 1198, 1200, 1201, 1203, 1206, 1207, 1208, 1209, 1210, 1214, 1216, 1217, 1223, 1239, 1241, 1247, 1252, 1256, 1260, 1270, 1271, 1273, 1288, 1293, 1298, 1301, 1303, 1305, 1306, 1307, 1316, 1317, 1324, 1325, 1327, 1331, 1333, 1334, 1338, 1344, 1346, 1347, 1348, 1349, 1354, 1358, 1359, 1360, 1361, 1363, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1374, 1376, 1380, 1384, 1386, 1387, 1389, 1394, 1395, 1397, 1400, 1408, 1416, 1418, 1422, 1423, 1425, 1428, 1435, 1436, 1437, 1438, 1440, 1442, 1446, 1448, 1449, 1451, 1453, 1460, 1461, 1466, 1478, 1482, 1484, 1488, 1494, 1495, 1497, 1500, 1520, 1523, 1525, 1527, 1528, 1529, 1530, 1536, 1537, 1540, 1541, 1544, 1546, 1548, 1550, 1551, 1556, 1557, 1558, 1560, 1563, 1566, 1568, 1570, 1571, 1573, 1575, 1577, 1581, 1583, 1586, 1592, 1593, 1594, 1595, 1596, 1599, 1600, 1601, 1613, 1623, 1624, 1625, 1627, 1629, 1633, 1634, 1640, 1643, 1644, 1645, 1649, 1650, 1653, 1657, 1660, 1662, 1664, 1669, 1677, 1679, 1687, 1688, 1690, 1695, 1697, 1698, 1701, 1711, 1715, 1717, 1719, 1722, 1724, 1727, 1732, 1734, 1735, 1748, 1750, 1755, 1756, 1760, 1762, 1767, 1770, 1776, 1777, 1781, 1786, 1790, 1791, 1793, 1797, 1802, 1805, 1806, 1807, 1814, 1822, 1825, 1826, 1827, 1828, 1837, 1840, 1841, 1842, 1844, 1845, 1857, 1859, 1862, 1864, 1866, 1871, 1872, 1874, 1875, 1881, 1884, 1890, 1893, 1902, 1905, 1908, 1909, 1911, 1914, 1915, 1921, 1923, 1924, 1925, 1926, 1927, 1930, 1933, 1934, 1935, 1937, 1938, 1940, 1942, 1943, 1944, 1948, 1949, 1950, 1957, 1962, 1966, 1970, 1973, 1975, 1979, 1980, 1988, 1990, 1994, 1997, 1998, 2003, 2004, 2006, 2007, 2009, 2011, 2015, 2017, 2018, 2019, 2020, 2023, 2024, 2027, 2029, 2030, 2032, 2037, 2039, 2041, 2042, 2047}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def BM25_score(query, index, N, doc_lengths, avg_doc_length, k1 = 1.2, b = 0.75):\n",
    "    bm25 = Counter()\n",
    "    for term in query:\n",
    "\n",
    "        # calculate idf\n",
    "        try:\n",
    "            df = index.df[term]\n",
    "        except:\n",
    "            continue\n",
    "        idf = math.log(N/df, 10)\n",
    "\n",
    "        # calculate bm25 score\n",
    "        pl = index.read_a_posting_list('', term)\n",
    "        for id, tf in pl:\n",
    "            try:\n",
    "                norm = (tf*(k1+1))/(tf+k1*(1-b+b*(doc_lengths[id]/avg_doc_length)))\n",
    "                bm25[id] += idf*norm\n",
    "            except:\n",
    "                pass\n",
    "    bm25_final = bm25.most_common()\n",
    "    return bm25_final"
   ],
   "metadata": {
    "id": "OLCJ9nFwjiif"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
