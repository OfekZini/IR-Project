{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":2,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-0016  GCE       2                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security → Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan 20 13:33 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-0016-m.c.ir-hw3-444616.internal:34573\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f4d86a7a760>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'hw3ir322' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if \"parquet\" in b.name:\n","#         print(b.name)\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"02f81c72"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":8,"id":"e4c523e7","metadata":{"id":"b1af29c9","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"f6375562"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":9,"id":"82881fbf","metadata":{"id":"d89a7a9a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":10,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":11,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":12,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"72bcf46a"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":19,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = ['category', 'references', 'also”', 'links', 'extrenal',\n","                 'first', 'see', 'new', 'two', 'list', 'may', 'one', 'district',\n","                 'including', 'became', 'however', 'com', 'many', 'began',\n","                 'make', 'made', 'part', 'would', 'people', 'second', 'also',\n","                 'following', 'history', 'thumb', 'external']\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","\n","def partition_postings_and_write(postings, base_dir):\n","  bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n","\n","  def write_bucket(b_w_pl):\n","      bucket_id, word_posting_pairs = b_w_pl\n","      return InvertedIndex.write_a_posting_list((bucket_id, list(word_posting_pairs)),base_dir,bucket_name)\n","\n","  posting_locs_list = bucket_rdd.map(write_bucket)\n","\n","  return posting_locs_list\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key = lambda x: x[0])\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda token: (token[0], len(token[1])))\n","\n","def tokenize(text):\n","  stemmer = PorterStemmer()\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  stemmed = [stemmer.stem(token) for token in tokens if token not in all_stopwords]\n","  return stemmed\n","\n","\n","\n","def stemmed_word_count(text, id):\n","  tokens = tokenize(text)\n","  token_counts = Counter(tokens)  # Calculate term frequencies efficiently\n","  res_list = [\n","        (token, (id, count))\n","        for token, count in token_counts.items()\n","        if token not in all_stopwords\n","    ]\n","  return res_list"]},{"cell_type":"code","execution_count":14,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","# word counts map\n","word_counts = doc_title_pairs.flatMap(lambda x: stemmed_word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","w2df = calculate_df(postings)\n","w2df_dict = w2df.collectAsMap()\n","\n"]},{"cell_type":"code","execution_count":20,"id":"fe420e8d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/01/20 20:55:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 18.0 (TID 1034) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000008/inverted_index_gcp.py\", line 182, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000008/inverted_index_gcp.py\", line 38, in __init__\n","    self._f = next(self._file_gen)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000008/inverted_index_gcp.py\", line 35, in <genexpr>\n","    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000008/inverted_index_gcp.py\", line 23, in _open\n","    return bucket.blob(path).open(mode)\n","AttributeError: 'Blob' object has no attribute 'open'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","25/01/20 20:55:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 18.0 (TID 1031) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000011/inverted_index_gcp.py\", line 182, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000011/inverted_index_gcp.py\", line 38, in __init__\n","    self._f = next(self._file_gen)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000011/inverted_index_gcp.py\", line 35, in <genexpr>\n","    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000011/inverted_index_gcp.py\", line 23, in _open\n","    return bucket.blob(path).open(mode)\n","AttributeError: 'Blob' object has no attribute 'open'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","25/01/20 20:55:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 18.0 (TID 1036) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 182, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 38, in __init__\n","    self._f = next(self._file_gen)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 35, in <genexpr>\n","    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 23, in _open\n","    return bucket.blob(path).open(mode)\n","AttributeError: 'Blob' object has no attribute 'open'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","25/01/20 20:55:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 18.0 (TID 1033) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000009/inverted_index_gcp.py\", line 182, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000009/inverted_index_gcp.py\", line 38, in __init__\n","    self._f = next(self._file_gen)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000009/inverted_index_gcp.py\", line 35, in <genexpr>\n","    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000009/inverted_index_gcp.py\", line 23, in _open\n","    return bucket.blob(path).open(mode)\n","AttributeError: 'Blob' object has no attribute 'open'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","25/01/20 20:55:25 ERROR org.apache.spark.scheduler.TaskSetManager: Task 3 in stage 18.0 failed 4 times; aborting job\n","25/01/20 20:55:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.3 in stage 18.0 (TID 1063) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 9): TaskKilled (Stage cancelled)\n","25/01/20 20:55:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.3 in stage 18.0 (TID 1057) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 9): TaskKilled (Stage cancelled)\n","25/01/20 20:55:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.2 in stage 18.0 (TID 1058) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 10): TaskKilled (Stage cancelled)\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 18.0 failed 4 times, most recent failure: Lost task 3.3 in stage 18.0 (TID 1055) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 182, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 38, in __init__\n    self._f = next(self._file_gen)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 35, in <genexpr>\n    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 23, in _open\n    return bucket.blob(path).open(mode)\nAttributeError: 'Blob' object has no attribute 'open'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 182, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 38, in __init__\n    self._f = next(self._file_gen)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 35, in <genexpr>\n    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 23, in _open\n    return bucket.blob(path).open(mode)\nAttributeError: 'Blob' object has no attribute 'open'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# partition posting lists and write out\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mpartition_postings_and_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpostings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:949\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    940\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    941\u001B[0m \u001B[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001B[39;00m\n\u001B[1;32m    942\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001B[39;00m\n\u001B[1;32m    947\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext) \u001B[38;5;28;01mas\u001B[39;00m css:\n\u001B[0;32m--> 949\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1308\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 18.0 failed 4 times, most recent failure: Lost task 3.3 in stage 18.0 (TID 1055) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 182, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 38, in __init__\n    self._f = next(self._file_gen)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 35, in <genexpr>\n    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 23, in _open\n    return bucket.blob(path).open(mode)\nAttributeError: 'Blob' object has no attribute 'open'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_68096/4038539762.py\", line 22, in write_bucket\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 182, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 38, in __init__\n    self._f = next(self._file_gen)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 35, in <genexpr>\n    self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1737379950114_0009/container_1737379950114_0009_01_000010/inverted_index_gcp.py\", line 23, in _open\n    return bucket.blob(path).open(mode)\nAttributeError: 'Blob' object has no attribute 'open'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]},{"name":"stderr","output_type":"stream","text":["25/01/20 20:55:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.1 in stage 18.0 (TID 1056) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 7): TaskKilled (Stage cancelled)\n","25/01/20 20:55:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 18.0 (TID 1062) (cluster-0016-w-1.c.ir-hw3-444616.internal executor 10): TaskKilled (Stage cancelled)\n","25/01/20 20:55:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.3 in stage 18.0 (TID 1059) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 7): TaskKilled (Stage cancelled)\n","25/01/20 20:55:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.3 in stage 18.0 (TID 1060) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 8): TaskKilled (Stage cancelled)\n","25/01/20 20:55:26 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.3 in stage 18.0 (TID 1061) (cluster-0016-w-0.c.ir-hw3-444616.internal executor 8): TaskKilled (Stage cancelled)\n"]}],"source":["# partition posting lists and write out\n","_ = partition_postings_and_write(postings, \"title\").collect()"]},{"cell_type":"code","execution_count":null,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":null,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_stemmed/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"8f880d59","metadata":{"id":"msogGbJ3c8JF","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["!gsutil ls -lh $index_dst"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}