{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2865fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "cluster-0016  GCE       2                                             RUNNING  us-central1-a\n",
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "166181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc7b78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ofekzini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "# from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d0e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /usr/lib/spark/jars/graph*: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800351a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37465804",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b418c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = 'hw3ir322' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "#         print(b.name)\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db889dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-21 23:15:09--  https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2503235912 (2.3G) [application/octet-stream]\n",
      "Saving to: 'pageviews-202108-user.bz2'\n",
      "\n",
      "eviews-202108-user.  63%[===========>        ]   1.48G  2.32MB/s    eta 4m 45s "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "p = Path(pv_path) \n",
    "pv_name = p.name\n",
    "pv_temp = f'{p.stem}-4dedup.txt'\n",
    "pv_clean = f'{p.stem}.pkl'\n",
    "# # Download the file (2.3GB) \n",
    "!wget -N $pv_path\n",
    "# # Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
    "# # total number of page views (5). Then, remove lines with article id or page \n",
    "# # view values that are not a sequence of digits.\n",
    "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
    "# # Create a Counter (dictionary) that sums up the pages views for the same \n",
    "# # article, resulting in a mapping from article id to total page views.\n",
    "wid2pv = Counter()\n",
    "with open(pv_temp, 'rt') as f:\n",
    "  for line in f:\n",
    "    parts = line.split(' ')\n",
    "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# # write out the counter as binary file (pickle it)\n",
    "# with open(pv_clean, 'wb') as f:\n",
    "#   pickle.dump(wid2pv, f)\n",
    "# with open(pv_clean, 'rb') as f:\n",
    "#   wid2pv = pickle.loads(f.read())\n",
    "page_view_dict = defaultdict(int)\n",
    "for doc_id, view in wid2pv.items():\n",
    "  page_view_dict[doc_id] = view\n",
    "\n",
    "with open(\"pageview.pkl\", 'wb') as f:\n",
    "  pickle.dump(page_view_dict, f)\n",
    "\n",
    "# bucket = client.bucket(bucket_name)\n",
    "# blob = bucket.blob('pv/pageview.pkl')\n",
    "# blob.upload_from_filename('pageview.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c4d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
